# data and label
x1 = [ 73.,  93.,  89.,  96.,  73.] → x1 데이터 값
x2 = [ 80.,  88.,  91.,  98.,  66.] → x2 데이터 값
x3 = [ 75.,  93.,  90., 100.,  70.] → x3 데이터 값
Y  = [152., 185., 180., 196., 142.] → y 값

# random weights
w1 = tf.Variable(tf.random.normal([1]))
w2 = tf.Variable(tf.random.normal([1]))
w3 = tf.Variable(tf.random.normal([1]))g
b  = tf.Variable(tf.random.normal([1]))

→ 초기값 1로 설정

learning_rate = 0.000001 → 작은 상수값

for i in range(1000+1): → 학습을 1001번 실행
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape: 
    → tape에 변수를 기록
        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b
        cost = tf.reduce_mean(tf.square(hypothesis - Y)) → 가설에서 실제값을 뺀 즉 오차 오차제곱의 평균값으로 cost정의
    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b]) → 4개의 변수들을 기울기 값을 구함
   
   GradientTape 사용 
    # update w1,w2,w3 and b
    w1.assign_sub(learning_rate * w1_grad) → w1 업데이트
    w2.assign_sub(learning_rate * w2_grad) → w2 업데이트
    w3.assign_sub(learning_rate * w3_grad) → w3 업데이트
    b.assign_sub(learning_rate * b_grad)   → b 업데이트

→ 경사값을 통해서 웨이트값을 지속적으로 업데이트 하는 내용입니다.
    
    if i % 50 == 0:
      print("{:5} | {:12.4f}".format(i, cost.numpy()))
    
    → 50번 마다 출력합니다.

→ 결과값

    0 | 5793889.5000
   50 |   64291.1484
  100 |     715.2902
  150 |       9.8462
  200 |       2.0152
  250 |       1.9252
  300 |       1.9210
  350 |       1.9177
  400 |       1.9145
  450 |       1.9114
  500 |       1.9081
  550 |       1.9050
  600 |       1.9018
  650 |       1.8986
  700 |       1.8955
  750 |       1.8923
  800 |       1.8892
  850 |       1.8861
  900 |       1.8829
  950 |       1.8798
 1000 |       1.8767
 
 → cost가 일정으로 줄고나서는 줄어들지 않는다. 
 
 data = np.array([
    # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)

# slice data
X = data[:, :-1] → 5행 3열 인풋
y = data[:, [-1]] → 5행 1열 출력 

W = tf.Variable(tf.random_normal([3, 1])) → 웨이트는 3행1  출력은 1개 x데이터의 컬럼갯수와 동일해야함 
b = tf.Variable(tf.random_normal([1])) → -1

learning_rate = 0.000001 → 작은 상수값

# hypothesis, prediction function
def predict(X): 
    return tf.matmul(X, W) + b

print("epoch | cost")

n_epochs = 2000
for i in range(n_epochs+1): → 학습을 2001번 실행
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
    → tape에 변수를 기록
        cost = tf.reduce_mean((tf.square(predict(X) - y)))  → 가설에서 실제값을 뺀 즉 오차 오차제곱의 평균값으로 cost정의

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b]) → 2개의 변수들을 기울기 값을 구함
   
    # updates parameters (W and b)
    W.assign_sub(learning_rate * W_grad) → w값 업데이트
    b.assign_sub(learning_rate * b_grad) → b값 업데이트
   
    
    if i % 100 == 0: → 100번 마다 출력합니다.
        print("{:5} | {:10.4f}".format(i, cost.numpy()))
        epoch | cost
        
  → 결과값
  
    0 |  5455.5903
  100 |    31.7443
  200 |    30.9326
  300 |    30.7894
  400 |    30.6468
  500 |    30.5055
  600 |    30.3644
  700 |    30.2242
  800 |    30.0849
  900 |    29.9463
 1000 |    29.8081
 1100 |    29.6710
 1200 |    29.5348
 1300 |    29.3989
 1400 |    29.2641
 1500 |    29.1299
 1600 |    28.9961
 1700 |    28.8634
 1800 |    28.7313
 1900 |    28.5997
 2000 |    28.4689
 
 → 처음엔 cost 값이 크지만 급속히 줄어서 더이상 줄어들지 않음 즉 최적화가 빠르게 이뤄짐
