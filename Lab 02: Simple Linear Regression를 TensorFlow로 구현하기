import tensorflow as tf
import numpy as np
tf.enable_eager_execution()

→ tensorflow를 tf로 가져 오기
numpy라는 패키지를 불러오면서 코드를 쓸 때 numpy를 np로 사용하기
execution을 활성화해서 즉시 실행합니다.

# Data
x_data = [1, 2, 3, 4, 5] 
y_data = [1, 2, 3, 4, 5]
→ x값 데이터 , y값 데이터

# W, b initialize 
W = tf.Variable(2.9)
b = tf.Variable(0.5)
→ 초기 w,b 값 지정

# W, b update
for i in range(100): → 학습을 100번 수행
    # Gradient descent (경사 하강법) minimize cost(W,b)를 찾는 알고리즘
    with tf.GradientTape() as tape:
        hypothesis = W * x_data + b 
        → 변수들의 변화를 tape에 기록
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))
    W_grad, b_grad = tape.gradient(cost, [W, b]) → 변수들의 w,b 경사도값 (미분값)을 구함
    → Gradient descent (경사 하강법) 알고리즘을 통해서 w와 b값을 지속적으로 업데이트 
    W.assign_sub(learning_rate * W_grad) → 알고리즘으로 통해서 w 업데이트
    b.assign_sub(learning_rate * b_grad) → 알고리즘으로 통해서 b 업데이트
    if i % 10 == 0:
      print("{:5}|{:10.4f}|{:10.4f}|{:10.6f}".format(i, W.numpy(), b.numpy(), cost))
    → 10번에 1번씩 w와 b값이 어떻게 변해가는지 출력
      
print()

 # predict
print(W * 5 + b) 
print(W * 2.5 + b)
→ 새로운 데이터 값

결과값
    i        w          b       cost
    0|    2.4520|    0.3760| 45.660004
   10|    1.1036|    0.0034|  0.206336
   20|    1.0128|   -0.0209|  0.001026
   30|    1.0065|   -0.0218|  0.000093
   40|    1.0059|   -0.0212|  0.000083
   50|    1.0057|   -0.0205|  0.000077
   60|    1.0055|   -0.0198|  0.000072
   70|    1.0053|   -0.0192|  0.000067
   80|    1.0051|   -0.0185|  0.000063
   90|    1.0050|   -0.0179|  0.000059
   
→ w값 2.9에서 시작해서 1.0으로 수렴 b값 0.5에서 시작해서 -0.017에 가까이 수렴 0에 가까움 즉 거의 정확함 
약간의 오차는 있지만 cost값도 0에 가까워짐 모델이 실제값을 예측하는데 잘 맞게됨 

tf.Tensor(5.0066934, shape=(), dtype=float32) → 5대입 5에 가까운 값이 나옵니다.
tf.Tensor(2.4946523, shape=(), dtype=float32) → 2.5대입 2.5에 근사한 값이 나옵니다.
